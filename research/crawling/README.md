# 크롤링(crawling) 작업

이 작업에서 우리가 궁극적으로 만들어 내야 할 결과물은, 각 `검색사이트`에서 가져온 `실시간 인기 검색어`로 실제 검색을 하여 찾아낸 `뉴스 URL`들의 `html 파일`들이다.

> 결정된 사항
>
> - 네이버, 다음, 구글, 트위터를 이용한다.
>
> - 각 사이트마다 10개씩의 실시간 인기 검색어를 수집한다.
>
> - 총 40개의 검색어로(중복 허용) 각 사이트 에서 뉴스 검색을 시도하여 상위 30개씩의 뉴스 URL을 수집한다.
>
> ```bash
> # 네 곳에서 키워드 수집 x 10개 검색어씩 x 네 곳에서 검색 x 30개 뉴스씩
> 4 * 10 * 4 * 30 = 4800 
> ```
>
> - 수집된 4800 개의 html 파일을 ***약속된 디렉토리 구조***로 저장한다. 

위에 결정된 사항대로, 각 검색사이트(4)에서 10개의 실시간 인기 검색어를 가져오고 각 검색어당 30개 씩의 뉴스를 각 사이트별(4)로 수집하면 최종 결과는 4800개의 뉴스가 된다.

현재 예상되기로는 각 사이트 마다 검색어를 가져오는 과정이 상이할 것이므로-지원하는 API, parsing 등의 차이로- 각 사이트마다 다른 모듈을 개발해야 할 것이다. 그러나 크게 복잡한 코드는 아닐 것이므로 크게 문제되지는 않을 것으로 예상된다.

## 예상되는 처리 과정

1. 검색사이트로부터 실시간 키워드 리스트 확보하기
2. 1의 결과로 실제 뉴스 검색을 하고 최신순으로 30개 URL 확보하기
3. 2의 결과로 URL을 html로 저장하여 약속된 경로에 저장하기

:point_right: *3번 과정은 검색사이트와 상관 없이 로직이 같으므로, 2번 작업까지만 완료하는 코드 를  검색사이트 별로 각각 작성 후, 추후에 각 코드를 병합하며 구현하는게 좋을 것으로 판단된다.*

## 약속된 경로

약속된 경로 형식은 간이로 아래와 같이 정하고 후에 또 토의하기로 한다.

> - 한 싸이클당 40개 키워드. 키워드당 120개 뉴스
> - 각 키워드와 뉴스파일은 어느 사이트의 키워드로 어느 사이트에서 검색한 것인지 알 수 있도록 Daum, Google, Naver, Twitter의 앞 자를 따서 suffix를 적용한다
> - 위 suffix 규칙을 따르면 추후 시각화 사이트를 고도화 할 때 중요한 메타 정보를 제공할 수 있을 것이다.

```bash
<어딘가>
   |-<YYMMDD>        
        |-<HHmm>	# 이 단위가 한 싸이클
       	    |-<D_K_01>
       	          |-<D_01>.html
                  |-...
                  |-<D_30>.html
                  ...
                  |-<G_30>.html
                  |-<N_01>.html
                  ...
                  |-<T_30>.html #총 120개 뉴스
            ...
            |-<D_K_10>
                  |-...
            |-<N_K_01>
                  |-<D_01>.html
                  |-...
            ...
            |-<D_K_01>
                  |-<D_01>.html
                  ...
            ...
            |-<T_K_10>              #총 40개 키워드
                  |-<D_01>.html
           ...  ...
        |-<HHmm>       # 다음 싸이클에 생성될 단위
            |-<D_K_01>
       ... ...
   ...
```

## 업무 분장

맡은 사이트 별로 다음 과정 까지만 구현한다.

> 1. 맡은 사이트 실시간 검색어 10개 수집
> 2. 1을 이용하여 맡은 사이트에서 뉴스검색 하여 상위 30개 URL 추출.

각각 맡은 사이트에 대하여 위 과정을 수행하는 코드를 작성하고나면, 모든 사이트에서 뉴스를 수집하는 최종 코드를 만들어낼 준비가 완료될 것으로 예상된다. 그러므로 이후 각각이 만들어낸 코드들을 조합하여 최종코드를 만들면 될 것이다.

회의결과 모두 python 으로 작성하기로 협의하였고, 분장된 업무는 다음과 같다

- 1조
  - 이희수: 다음
  - 문진한: 구글
- 2조
  - 박종선: 네이버
  - 김재현: 트위터

## 최종 코드 예상 형식

각 사이트 용으로 구현된 코드를 합치고, 수집된 정보들을 약속된 경로 형식으로 저장하는 `최종 코드`의 형식은 아래와 같을 것으로 예상된다.

```pseudocode
A가 만든 코드로 URL 리스트  listA 확보
B가 만든 코드로 URL 리스트  listB 확보
...
모두 합쳐 listAll 확보 // listAll = [listA, listB, ...]

for list in listAll
	for elem in list
		file = download(elem)
		저장 로직에 따라 file 저장
```

## 각 업무분장 단위코드 최종 결과물 예시

각각 분장하여 작업한 코드들의 최종 결과물은 아래 예시와 같이 하기로 결정하였다.

```
[
	{
		'keyword': '에어핏 1&1 대란', 
		'items': [
			{
				'title': "에어핏 1&amp;1 대란, 오퀴즈 이천만원이벤트 오전 11시 문제…'ㅇㄹㅂㄷㅌㄱ' 초...", 
				'link': 'http://www.joongboo.com/news/articleView.html?idxno=363364790'
			}, 
			{
				'title': "캐시슬라이드 '에어핏 1&amp;1 대란', 오전 11시 'ㅁㄹㅂㅅㅋㅍ' 정답 공개", 
				'link': 'http://www.seoulwire.com/news/articleView.html?idxno=232347'
			},
			{
				'title': '에어핏 1&amp;1 대란, 오퀴즈 11시 ㅇㄹㅂㄷㅌㄱ 정답은?',
				'link': 'https://news.naver.com/main/read.nhnode=LSD&mid=sec&sid1=105&oid=417&aid=0000457485'
			}
		]
	}, 
	{
		'keyword': '에어핏1&1', 
		'items': [
			{
				'title': "에어핏 1&amp;1 대란, 오퀴즈 이천만원이벤트 오전 11시 문제…'ㅇㄹㅂㄷㅌㄱ' 초...",
				'link': 'http://www.joongboo.com/news/articleView.html?idxno=363364790'
			},
			{
				'title': "캐시슬라이드 '에어핏 1&amp;1 대란', 오전 11시 'ㅁㄹㅂㅅㅋㅍ' 정답 공개",
				'link': 'http://www.seoulwire.com/news/articleView.html?idxno=232347'
			},
			{
				'title': '에어핏 1&amp;1 대란, 오퀴즈 11시 ㅇㄹㅂㄷㅌㄱ 정답은?', 'link': 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=105&oid=417&aid=0000457485'
			}
		]
	}, 
	{
		'keyword': '유병재 8억칫솔',
		'items': [
			{
				'title': '유병재 8억칫솔, ‘ㅊㅇㅁㅂㄱ’ 퀴즈 정답은?',
				'link': 'https://news.naver.com/main/read.nhn?mode=LSD&
mid=sec&sid1=102&oid=081&aid=0003034383'
			},
			{
				'title': '유병재 8억칫솔, ㅂㄹㄹㅇㅌ 초성퀴즈 정답은?',
				'link': 'http://www.kyeonggi.com/news/articleView.html?idxno=2175314'
			},
			{
				'title': "'유병재 8억칫솔' 초성퀴즈 등장…정답은?",
				'link': 'http://www.yeongnam.com/mnews/newsview.do?mode=newsView&amp;newskey=20191008.990011104361242'
			}
		]
	}
]
```

## 추후 작업

이 모든 과정이 완료 되면 html 파일을 전처리하여 순수 text 만으로 구성된 파일로 변환하는 코드를 작성해야한다. 그 부분은 다른 작업단위로 분리해서 진행하도록 한다.