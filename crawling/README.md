# 크롤링(crawling) 작업

이 작업에서 우리가 궁극적으로 만들어 내야 할 결과물은, 각 `검색사이트`에서 가져온 `실시간 인기 검색어`로 실제 검색을 하여 찾아낸 `뉴스 URL`들의 `html 파일`들이다.

예를 들어 한 검색사이트에서 10개의 실시간 인기 검색어를 가져오고 각 검색어당 30개 씩의 뉴스를 수집하기로 했다면 최종결과물은 목표 디렉토리 한 곳에 저장된 300개의 html 파일들이다.

현재 예상되기로는 각 사이트 마다 검색어를 가져오는 과정이 상이할 것이므로-지원하는 API, parsing 등의 차이로- 각 사이트마다 다른 모듈을 개발해야 할 것이다. 그러나 크게 복잡한 코드는 아닐 것이므로 크게 문제되지는 않을 것으로 예상된다.

## 예상되는 처리 과정

1. 검색사이트로부터 실시간 키워드 리스트 확보하기
2. 1의 결과로 실제 뉴스 검색을 하고 최신순으로 30개 URL 확보하기
3. 2의 결과로 URL을 html로 저장하여 약속된 경로에 저장하기

:point_right: *3번 과정은 검색사이트와 상관 없이 로직이 같으므로, 2번 작업까지만 완료하는 코드 를  검색사이트 별로 각각 작성 후, 추후에 각 코드를 병합하며 구현하는게 좋을 것으로 판단된다.*

## 약속된 경로

약속된 경로 형식은 간이로 아래와 같이 정하고 후에 또 토의 하기로 한다.

```bash
<어딘가>
   |-<날짜시간분형식>        #이 단위가 한 싸이클
       |-<검색사이트1>
       	     |-<키워드명1>
       	          |-<뉴스제목1>.html
                  |-<뉴스제목2>.html
                  |-...
                  |-<뉴스제목30>.thml
             |-<키워드명2>
                  |-<뉴스제목1>.html
                  |-...
             ...
       |-<검색사이트2>
             |-<키워드명1>
                  |-<뉴스제목1>.html
       ...  ...  ...
    |-<날짜시간분형식>       # 다음 싸이클에 생성될 단위
   ...        
```

## 최종 코드 예상 형식

각 사이트용으로 구현된 코드를 합치고 약속된 경로 형식으로 저장하는 최종 코드의 형식은 아래와 같을것으로 예상된다.

```pseudocode
A가 만든 코드로 URL 리스트  listA 확보
B가 만든 코드로 URL 리스트  listB 확보
...
모두 합쳐 listAll 확보 // listAll = [listA, listB, ...]

for list in listAll
	for elem in list
		file = download(elem)
		저장 로직에 따라 file 저장
```

## 추후 작업

이 모든 과정이 완료 되면 html 파일을 분석하기에 문제가 없는 txt파일로 전처리 하는 작업을 진행야하는데, 그 부분은 다른 작업단위로 빼서 진행하도록 한다.

